{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYaLwXvi3vNipjJLsYXZ6n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/localecho/Python-GUI-examples/blob/chinesewall/Copy_of_Copy_of_crewai_jobprep_v1_012224.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVf-PqESLOob",
        "outputId": "1983fa2e-1799-4d45-a764-ce65e94f7a09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting crewai\n",
            "  Downloading crewai-0.1.32-py3-none-any.whl (25 kB)\n",
            "Collecting langchain<0.2.0,>=0.1.0 (from crewai)\n",
            "  Downloading langchain-0.1.3-py3-none-any.whl (803 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.6/803.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-openai<0.0.3,>=0.0.2 (from crewai)\n",
            "  Downloading langchain_openai-0.0.2.post1-py3-none-any.whl (28 kB)\n",
            "Collecting openai<2.0.0,>=1.7.1 (from crewai)\n",
            "  Downloading openai-1.9.0-py3-none-any.whl (223 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.4/223.4 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic<3.0.0,>=2.4.2 (from crewai)\n",
            "  Downloading pydantic-2.5.3-py3-none-any.whl (381 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.9/381.9 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.0->crewai) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.0->crewai) (2.0.24)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.0->crewai) (3.9.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.0->crewai) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain<0.2.0,>=0.1.0->crewai)\n",
            "  Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain<0.2.0,>=0.1.0->crewai)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.14 (from langchain<0.2.0,>=0.1.0->crewai)\n",
            "  Downloading langchain_community-0.0.15-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2,>=0.1.14 (from langchain<0.2.0,>=0.1.0->crewai)\n",
            "  Downloading langchain_core-0.1.15-py3-none-any.whl (229 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.6/229.6 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.1,>=0.0.83 (from langchain<0.2.0,>=0.1.0->crewai)\n",
            "  Downloading langsmith-0.0.83-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.0->crewai) (1.23.5)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.0->crewai) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.0->crewai) (8.2.3)\n",
            "Collecting tiktoken<0.6.0,>=0.5.2 (from langchain-openai<0.0.3,>=0.0.2->crewai)\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.7.1->crewai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.7.1->crewai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai<2.0.0,>=1.7.1->crewai)\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.7.1->crewai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.7.1->crewai) (4.66.1)\n",
            "Collecting typing-extensions<5,>=4.7 (from openai<2.0.0,>=1.7.1->crewai)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Collecting annotated-types>=0.4.0 (from pydantic<3.0.0,>=2.4.2->crewai)\n",
            "  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
            "Collecting pydantic-core==2.14.6 (from pydantic<3.0.0,>=2.4.2->crewai)\n",
            "  Downloading pydantic_core-2.14.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.0->crewai) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.0->crewai) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.0->crewai) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.0->crewai) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.0->crewai) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.7.1->crewai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.7.1->crewai) (1.2.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.0->crewai)\n",
            "  Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.0->crewai)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.7.1->crewai) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai<2.0.0,>=1.7.1->crewai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.7.1->crewai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain<0.2.0,>=0.1.0->crewai)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.14->langchain<0.2.0,>=0.1.0->crewai) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.0->crewai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.0->crewai) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain<0.2.0,>=0.1.0->crewai) (3.0.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<0.6.0,>=0.5.2->langchain-openai<0.0.3,>=0.0.2->crewai) (2023.6.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.0->crewai)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: typing-extensions, mypy-extensions, marshmallow, jsonpointer, h11, annotated-types, typing-inspect, tiktoken, pydantic-core, jsonpatch, httpcore, pydantic, httpx, dataclasses-json, openai, langsmith, langchain-core, langchain-openai, langchain-community, langchain, crewai\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.14\n",
            "    Uninstalling pydantic-1.10.14:\n",
            "      Successfully uninstalled pydantic-1.10.14\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed annotated-types-0.6.0 crewai-0.1.32 dataclasses-json-0.6.3 h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.3 langchain-community-0.0.15 langchain-core-0.1.15 langchain-openai-0.0.2.post1 langsmith-0.0.83 marshmallow-3.20.2 mypy-extensions-1.0.0 openai-1.9.0 pydantic-2.5.3 pydantic-core-2.14.6 tiktoken-0.5.2 typing-extensions-4.9.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "# Environment Setup\n",
        "!pip install crewai\n",
        "\n",
        "# Importing necessary libraries for CrewAI\n",
        "import os\n",
        "\n",
        "from crewai import Agent, Task, Crew, Process\n",
        "from langchain.agents import Tool\n",
        "from langchain.utilities import GoogleSerperAPIWrapper\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Setting up environment variables for API keys\n",
        "# Replace 'Your_Key' with your actual API keys\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "os.environ[\"SERPER_API_KEY\"] = \"4e153c0ec14a8c5726bfcd2a8f0ea30c8073cb92\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_context_and_question(topic):\n",
        "    # Define a template for context and question\n",
        "    context_template = f\"\"\"\n",
        "    [This is a background about {topic}. It includes key information,\n",
        "    historical aspects, recent developments, and crucial factors related to {topic}.]\n",
        "    \"\"\"\n",
        "\n",
        "    question_template = f\"\"\"\n",
        "    [What are the main challenges and opportunities associated with {topic},\n",
        "    considering the current trends and future prospects?]\n",
        "    \"\"\"\n",
        "\n",
        "    # Return the formatted context and question\n",
        "    return context_template, question_template\n",
        "\n",
        "# Example usageto\n",
        "topic = \"\"\"Objective: Create a sophisticated flowchart that outlines the development process of Personalized Education Plans:\n",
        "    - 'Design a learning path for a [age]-year-old interested in [subject], with an emphasis on [specific skill or topic].'\n",
        "    This flowchart needs to be detailed, showing the delegation of tasks across time, people, processes, and technology, using advanced mermaid features like hierarchical and non-hierarchical bucketing and colors.\n",
        "\n",
        "Constraints:\n",
        "- The flowchart must be intuitive and well-structured.\n",
        "- Must include narrative elements for engagement.\n",
        "- Mermaid code should be well-documented, maintainable, and embeddable in project management tools.\n",
        "- The theme of the fashion line should be clearly represented.\n",
        "- Must involve various roles: Creative Writing, Fashion Design, Business Strategy, AI-Driven Marketing, and Automated Code Debugging.\n",
        "\n",
        "Essential Information:\n",
        "- The flowchart starts with narrative development and ends with the final product launch.\n",
        "- It must cover the entire process from concept to market entry.\n",
        "- Roles and responsibilities need to be clearly defined and integrated into the flowchart.\n",
        "\n",
        "Identify Pitfalls:\n",
        "- Overcomplexity that could lead to confusion.\n",
        "- Failing to clearly represent the theme and the roles involved.\n",
        "- Lack of clarity in the narrative and the process flow.\n",
        "- Technical errors in the mermaid code.\n",
        "\n",
        "Consider Improvements:\n",
        "- Utilize hierarchical bucketing to organize roles and processes clearly.\n",
        "- Use color coding to differentiate between stages of development and roles.\n",
        "- Ensure the narrative is engaging and seamlessly integrates with the flowchart.\n",
        "- Prioritize readability and ease of understanding in the mermaid code.\"\"\"\n",
        "\n",
        "print(topic)\n",
        "\n",
        "\n",
        "\"##\n",
        "context, question = generate_context_and_question(topic)\n",
        "\n",
        "# Display the context and question\n",
        "print(\"Context:\\n\", context)\n",
        "print(\"\\nQuestion:\\n\", question)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "Qy9J74ToL-E7",
        "outputId": "9d661f6f-0f9c-4967-d1eb-0948d1b57902"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 48) (<ipython-input-1-e676402c0880>, line 48)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-e676402c0880>\"\u001b[0;36m, line \u001b[0;32m48\u001b[0m\n\u001b[0;31m    \"##\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 48)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "from nltk.util import ngrams\n",
        "import torch\n",
        "\n",
        "def extract_themes_pro(context, question, num_themes=100, ngram_range=(1, 3)):\n",
        "    # Load pre-trained BERT model\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    # Combine context and question\n",
        "    context = ''\n",
        "    question = ''\n",
        "    #need to clean cell\n",
        "    text = #context + \" \" + question\n",
        "\n",
        "    # Generate n-grams\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    n_grams = list(ngrams(tokens, ngram_range[1], pad_left=True, pad_right=True))\n",
        "    n_grams = [' '.join(gram).strip() for gram in n_grams if None not in gram]\n",
        "\n",
        "    # Generate embeddings\n",
        "    embeddings = []\n",
        "    for gram in n_grams:\n",
        "        inputs = tokenizer(gram, return_tensors='pt')\n",
        "        outputs = model(**inputs)\n",
        "        embeddings.append(outputs.last_hidden_state.mean(dim=1).detach().numpy())\n",
        "\n",
        "    embeddings = np.vstack(embeddings)\n",
        "\n",
        "    # Calculate distances\n",
        "    dist_matrix = euclidean_distances(embeddings)\n",
        "\n",
        "    # Extract themes based on the closest distances\n",
        "    themes = []\n",
        "    for idx, row in enumerate(dist_matrix):\n",
        "        closest_indices = np.argsort(row)[:num_themes]\n",
        "        closest_ngrams = [n_grams[i] for i in closest_indices]\n",
        "        themes.extend(closest_ngrams)\n",
        "\n",
        "    # Ensure uniqueness and return\n",
        "    unique_themes = list(set(themes))\n",
        "    return unique_themes[:num_themes]"
      ],
      "metadata": {
        "id": "xXai3dIgqz_g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "78d48e74-b3c8-4c06-babf-65704a86f989"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-3-a949fc61d0d8>, line 16)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-a949fc61d0d8>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    text = #context + \" \" + question\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Ensure NLTK resources are available\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def extract_themes_pro_bert(context, question, num_themes=1000):\n",
        "    \"\"\"\n",
        "    Analyze the context and question to extract key themes using BERT embeddings and multi-dimensional Euclidean distances.\n",
        "\n",
        "    Args:\n",
        "    context (str): The context or background information.\n",
        "    question (str): The specific question or problem statement.\n",
        "    num_themes (int): Number of themes to extract.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of the most prominent themes.\n",
        "    \"\"\"\n",
        "    # Initialize BERT tokenizer and model\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    # Combine context and question\n",
        "    text = context + \" \" + question\n",
        "\n",
        "    # Tokenize and remove stopwords\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words]\n",
        "\n",
        "    # Convert tokens to BERT embeddings\n",
        "    inputs = tokenizer(filtered_tokens, return_tensors='pt', padding=True, truncation=True)\n",
        "    outputs = model(**inputs)\n",
        "    embeddings = outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
        "\n",
        "    # Calculate Euclidean distances between embeddings\n",
        "    dist_matrix = euclidean_distances(embeddings)\n",
        "\n",
        "    # Extract themes based on the closest distances\n",
        "    idx_closest = np.argsort(dist_matrix, axis=1)[:, :num_themes]\n",
        "    themes = [filtered_tokens[i] for i in idx_closest.flatten()]\n",
        "\n",
        "    # Ensure uniqueness and return\n",
        "    unique_themes = list(set(themes))\n",
        "    return unique_themes[:num_themes]\n"
      ],
      "metadata": {
        "id": "Uf5k2hN5xXBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def match_job_titles(themes):\n",
        "    \"\"\"\n",
        "    Match extracted themes with appropriate job titles.\n",
        "\n",
        "    Args:\n",
        "    themes (list): List of themes extracted from text.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of matched job titles.\n",
        "    \"\"\"\n",
        "    # Example job titles categorization (can be expanded based on the resources)\n",
        "    job_titles = {\n",
        "        \"marketing\": [\"Digital Marketing Director\", \"Senior Marketing Strategist\"],\n",
        "        \"cybersecurity\": [\"Security Analyst\", \"Information Security Manager\"],\n",
        "        \"data science\": [\"Data Scientist\", \"Data Analyst\"],\n",
        "        \"finance\": [\"Financial Analyst\", \"Corporate Finance Manager\"],\n",
        "        \"sales\": [\"Sales Manager\", \"Account Executive\"],\n",
        "        \"hr\": [\"HR Manager\", \"Talent Acquisition Specialist\"],\n",
        "        # Add more categories and titles as needed\n",
        "    }\n",
        "\n",
        "    matched_titles = []\n",
        "    for theme in themes:\n",
        "        # Example logic to match themes with job titles\n",
        "        # This can be refined based on the specific themes and available job titles\n",
        "        category = theme.lower()\n",
        "        if category in job_titles:\n",
        "            matched_titles.extend(job_titles[category])\n",
        "        else:\n",
        "            # Handling unmatched themes\n",
        "            matched_titles.append(\"Specialist in \" + theme.title())\n",
        "\n",
        "    return matched_titles\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "q3qQ1fxrXKlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assemble the AI Crew\n",
        "from crewai import Agent, Task, Crew, Process\n",
        "import random\n",
        "\n",
        "# Function to analyze context and question for key themes\n",
        "\n",
        "# Extracting themes\n",
        "themes_bert = extract_themes_pro_bert(context, question)\n",
        "themes_pro =  extract_themes_pro(context, question)\n",
        "themes = themes_bert + themes_pro\n",
        "\n",
        "\n",
        "# Number of agents to create\n",
        "num_agents = 100\n",
        "\n",
        "# Creating a list to hold all agents\n",
        "agents = []\n",
        "# Loop to create each agent\n",
        "for i in range(num_agents):\n",
        "    theme_index = i % len(themes)\n",
        "    role = f'Agent with expertise in {themes[theme_index]}'\n",
        "    goal = f'Provide insights on {themes[theme_index]} related to the question'\n",
        "    backstory = f'Agent {i+1} specializing in {themes[theme_index]}.'\n",
        "\n",
        "    # Creating a wildcard agent with contrasting characteristics\n",
        "    if i == num_agents - 1:\n",
        "        role = 'Wildcard Agent with Diverse Perspective'\n",
        "        goal = 'Challenge the conventional thinking and provide alternative viewpoints'\n",
        "        backstory = 'An agent known for unconventional approaches and thinking outside the box.'\n",
        "\n",
        "    # Creating an agent\n",
        "    agent = Agent(\n",
        "        role=role,\n",
        "        goal=goal,\n",
        "        backstory=backstory,\n",
        "        verbose=True\n",
        "    )\n",
        "    agents.append(agent)\n",
        "\n",
        "# Displaying the created agents\n",
        "for agent in agents:\n",
        "    print(f'Agent Role: {agent.role}, Goal: {agent.goal}, Backstory: {agent.backstory}')\n"
      ],
      "metadata": {
        "id": "eIWiASsgMKWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assemble the AI Crew\n",
        "from crewai import Agent, Task, Crew, Process\n",
        "import random\n",
        "\n",
        "# Function to analyze context and question for key themes\n",
        "\n",
        "# Extracting themes\n",
        "themes_bert = extract_themes_pro_bert(context, question)\n",
        "themes_pro = extract_themes_pro(context, question)\n",
        "themes = themes_bert + themes_pro\n",
        "\n",
        "# Number of agents to create\n",
        "num_agents = 240\n",
        "\n",
        "# Creating a list to hold all agents\n",
        "agents = []\n",
        "# Loop to create each agent\n",
        "for i in range(num_agents):\n",
        "    theme_index = i % len(themes)\n",
        "    role = f'Agent with expertise in {themes[theme_index]}'\n",
        "    goal = f'Provide insights on {themes[theme_index]} related to the question'\n",
        "    backstory = f'Agent {i+1} specializing in {themes[theme_index]}.'\n",
        "\n",
        "    # Creating a wildcard agent with contrasting characteristics\n",
        "    if i == num_agents - 1:\n",
        "        role = 'Wildcard Agent with Diverse Perspective'\n",
        "        goal = 'Challenge the conventional thinking and provide alternative viewpoints'\n",
        "        backstory = 'An agent known for unconventional approaches and thinking outside the box.'\n",
        "\n",
        "    # Creating an agent\n",
        "    agent = Agent(\n",
        "        role=role,\n",
        "        goal=goal,\n",
        "        backstory=backstory,\n",
        "        verbose=True\n",
        "    )\n",
        "    agents.append(agent)\n",
        "\n",
        "# Displaying the created agents\n",
        "for agent in agents:\n",
        "    print(f'Agent Role: {agent.role}, Goal: {agent.goal}, Backstory: {agent.backstory}')\n"
      ],
      "metadata": {
        "id": "vXyD_Q46cqPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute crew to perform the tasks\n",
        "\n",
        "# Define a task for the agents\n",
        "# This should be defined according to the specific problem at hand\n",
        "task = Task(description= question, agent=agents[0])\n",
        "\n",
        "# Create the crew with a sequential process\n",
        "crew = Crew(\n",
        "    agents=agents,\n",
        "    tasks=[task],\n",
        "    process=Process.sequential,  # Using sequential process, but can be adapted\n",
        "    verbose=True\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "Ymg9kdIyMM5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Initialize metrics\n",
        "start_time = time.time()\n",
        "error_count = 0\n",
        "total_tasks = len(crew.tasks)  # Assuming 'crew' is your CrewAI instance\n",
        "\n",
        "# Execute CrewAI tasks\n",
        "try:\n",
        "    result = crew.kickoff()\n",
        "except Exception as e:\n",
        "    error_count += 1\n",
        "    print(f\"Error occurred: {e}\")\n",
        "\n",
        "# Calculate total time taken\n",
        "end_time = time.time()\n",
        "total_time = end_time - start_time\n",
        "\n",
        "# Output results\n",
        "print(f\"Total tasks: {total_tasks}\")\n",
        "print(f\"Errors: {error_count}\")\n",
        "print(f\"Total time taken: {total_time} seconds\")\n"
      ],
      "metadata": {
        "id": "WIOEjqr5Px9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "i1uByeei-fRu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Initialize metrics\n",
        "start_time = time.time()\n",
        "error_count = 0\n",
        "total_tasks = len(crew.tasks)  # Assuming 'crew' is your CrewAI instance\n",
        "\n",
        "# Execute CrewAI tasks\n",
        "try:\n",
        "    result = crew.kickoff()\n",
        "except Exception as e:\n",
        "    error_count += 1\n",
        "    print(f\"Error occurred: {e}\")\n",
        "\n",
        "# Calculate total time taken\n",
        "end_time = time.time()\n",
        "total_time = end_time - start_time\n",
        "\n",
        "# Output results\n",
        "print(f\"Total tasks: {total_tasks}\")\n",
        "print(f\"Errors: {error_count}\")\n",
        "print(f\"Total time taken: {total_time} seconds\")\n"
      ],
      "metadata": {
        "id": "HspaX32w-iC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "√"
      ],
      "metadata": {
        "id": "w8rzTopq-n-H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "he prospects of data science web applet plotly opportunities are vast. Data science is a rapidly growing field, and tools like Plotly are in demand for their ability to create interactive and visually appealing data visualizations. Here's a look at the future prospects:\n",
        "\n",
        "1. Increased Demand: The demand for data science and data visualization tools, like Plotly, is expected to increase. Companies are realizing the importance of data-driven decisions, which will lead to a rise in opportunities.\n",
        "\n",
        "2. Advanced Visualizations: As technology advances, we can expect to see more advanced, interactive, and dynamic visualizations. This will provide new opportunities for tools like Plotly to innovate and develop new features and products.\n",
        "\n",
        "3. Integration with AI: The integration of AI and machine learning with data visualization tools will be a big opportunity. This can lead to the creation of smarter and more insightful visualizations.\n",
        "\n",
        "4. Customization: There will be more opportunities for customization. Companies will want tools that can be tailored to their specific needs, creating opportunities for more specialized versions of tools like Plotly.\n",
        "\n",
        "5. Accessibility: There will be a growing need for tools that are user-friendly and accessible to people without a deep understanding of data science. This will create opportunities for tools that can make data science accessible to a wider audience.\n",
        "\n",
        "Challenges that may come along the way include staying up to date with rapidly advancing trends and technologies, maintaining the balance between user-friendliness and advanced features, and ensuring data security and privacy. However, these challenges also represent opportunities for growth and innovation.Do I need to use a tool? No\n",
        "Final Answer: To answer the question about the main challenges and opportunities associated with building a 30-day marketing strategy to determine the top 10 MVP data science web applet plotly opportunities, we have to consider the following:\n",
        "\n",
        "Challenges include time constraints, market research, identifying key metrics, limited resources, customer acquisition, competition, rapid technological advancements, and user-friendly design.\n",
        "\n",
        "Opportunities include fast feedback, market positioning, learning opportunities, potential partnerships, customer engagement, interactive data visualization, real-time data analysis, machine learning integration, customization, collaboration, mobile compatibility, user-friendly interface, scalability, and education and training.\n",
        "\n",
        "The major current market trends are the growing demand for data science and data visualization tools, the potential for integration with other platforms, and the untapped educational sector.\n",
        "\n",
        "Looking at the future prospects, there is a predicted increased demand for data science and visualization tools. Other prospects include the development of more advanced, interactive, and dynamic visualizations, integration with AI, more opportunities for customization, and a greater need for user-friendly and accessible tools.\n",
        "\n",
        "To determine the top 10 MVP data science web applet plotly opportunities, factors such as the size of the potential market, the level of competition, the potential for growth, and the needs and preferences of the target audience should be considered.\n",
        "\n",
        "> Finished chain.\n",
        "\n",
        "[DEBUG]: [Agent with expertise in main] Task output: To answer the question about the main challenges and opportunities associated with building a 30-day marketing strategy to determine the top 10 MVP data science web applet plotly opportunities, we have to consider the following:\n",
        "\n",
        "Challenges include time constraints, market research, identifying key metrics, limited resources, customer acquisition, competition, rapid technological advancements, and user-friendly design.\n",
        "\n",
        "Opportunities include fast feedback, market positioning, learning opportunities, potential partnerships, customer engagement, interactive data visualization, real-time data analysis, machine learning integration, customization, collaboration, mobile compatibility, user-friendly interface, scalability, and education and training.\n",
        "\n",
        "The major current market trends are the growinghe prospects of data science web applet plotly opportunities are vast. Data science is a rapidly growing field, and tools like Plotly are in demand for their ability to create interactive and visually appealing data visualizations. Here's a look at the future prospects:\n",
        "\n",
        "1. Increased Demand: The demand for data science and data visualization tools, like Plotly, is expected to increase. Companies are realizing the importance of data-driven decisions, which will lead to a rise in opportunities.\n",
        "\n",
        "2. Advanced Visualizations: As technology advances, we can expect to see more advanced, interactive, and dynamic visualizations. This will provide new opportunities for tools like Plotly to innovate and develop new features and products.\n",
        "\n",
        "3. Integration with AI: The integration of AI and machine learning with data visualization tools will be a big opportunity. This can lead to the creation of smarter and more insightful visualizations.\n",
        "\n",
        "4. Customization: There will be more opportunities for customization. Companies will want tools that can be tailored to their specific needs, creating opportunities for more specialized versions of tools like Plotly.\n",
        "\n",
        "5. Accessibility: There will be a growing need for tools that are user-friendly and accessible to people without a deep understanding of data science. This will create opportunities for tools that can make data science accessible to a wider audience.\n",
        "\n",
        "Challenges that may come along the way include staying up to date with rapidly advancing trends and technologies, maintaining the balance between user-friendliness and advanced features, and ensuring data security and privacy. However, these challenges also represent opportunities for growth and innovation.Do I need to use a tool? No\n",
        "Final Answer: To answer the question about the main challenges and opportunities associated with building a 30-day marketing strategy to determine the top 10 MVP data science web applet plotly opportunities, we have to consider the following:\n",
        "\n",
        "Challenges include time constraints, market research, identifying key metrics, limited resources, customer acquisition, competition, rapid technological advancements, and user-friendly design.\n",
        "\n",
        "Opportunities include fast feedback, market positioning, learning opportunities, potential partnerships, customer engagement, interactive data visualization, real-time data analysis, machine learning integration, customization, collaboration, mobile compatibility, user-friendly interface, scalability, and education and training.\n",
        "\n",
        "The major current market trends are the growing demand for data science and data visualization tools, the potential for integration with other platforms, and the untapped educational sector.\n",
        "\n",
        "Looking at the future prospects, there is a predicted increased demand for data science and visualization tools. Other prospects include the development of more advanced, interactive, and dynamic visualizations, integration with AI, more opportunities for customization, and a greater need for user-friendly and accessible tools.\n",
        "\n",
        "To determine the top 10 MVP data science web applet plotly opportunities, factors such as the size of the potential market, the level of competition, the potential for growth, and the needs and preferences of the target audience should be considered.\n",
        "\n",
        "> Finished chain.\n",
        "\n",
        "[DEBUG]: [Agent with expertise in main] Task output: To answer the question about the main challenges and opportunities associated with building a 30-day marketing strategy to determine the top 10 MVP data science web applet plotly opportunities, we have to consider the following:\n",
        "\n",
        "Challenges include time constraints, market research, identifying key metrics, limited resources, customer acquisition, competition, rapid technological advancements, and user-friendly design.\n",
        "\n",
        "Opportunities include fast feedback, market positioning, learning opportunities, potential partnerships, customer engagement, interactive data visualization, real-time data analysis, machine learning integration, customization, collaboration, mobile compatibility, user-friendly interface, scalability, and education and training.\n",
        "\n",
        "The major current market trends are the growing demand for data science and data visualization tools, the potential for integration with other platforms, and the untapped educational sector.\n",
        "\n",
        "Looking at the future prospects, there is a predicted increased demand for data science and visualization tools. Other prospects include the development of more advanced, interactive, and dynamic visualizations, integration with AI, more opportunities for customization, and a greater need for user-friendly and accessible tools.\n",
        "\n",
        "To determine the top 10 MVP data science web applet plotly opportunities, factors such as the size of the potential market, the level of competition, the potential for growth, and the needs and preferences of the target audience should be considered.\n",
        "\n",
        "\n",
        "Total tasks: 1\n",
        "Errors: 0\n",
        "Total time taken: 221.04770803451538 seconds demand for data science and data visualization tools, the potential for integration with other platforms, and the untapped educational sector.\n",
        "\n",
        "Looking at the future prospects, there is a predicted increased demand for data science and visualization tools. Other prospects include the development of more advanced, interactive, and dynamic visualizations, integration with AI, more opportunities for customization, and a greater need for user-friendly and accessible tools.\n",
        "\n",
        "To determine the top 10 MVP data science web applet plotly opportunities, factors such as the size of the potential market, the level of competition, the potential for growth, and the needs and preferences of the target audience should be considered.\n",
        "\n",
        "\n",
        "Total tasks: 1\n",
        "Errors: 0\n",
        "Total time taken: 221.04770803451538 seconds"
      ],
      "metadata": {
        "id": "0XMoO1Gn-6RE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Initialize metrics\n",
        "start_time = time.time()\n",
        "error_count = 0\n",
        "total_tasks = len(crew.tasks)  # Assuming 'crew' is your CrewAI instance\n",
        "\n",
        "# Execute CrewAI tasks\n",
        "try:\n",
        "    result = crew.kickoff()\n",
        "except Exception as e:\n",
        "    error_count += 1\n",
        "    print(f\"Error occurred: {e}\")\n",
        "\n",
        "# Calculate total time taken\n",
        "end_time = time.time()\n",
        "total_time = end_time - start_time\n",
        "\n",
        "# Output results\n",
        "print(f\"Total tasks: {total_tasks}\")\n",
        "print(f\"Errors: {error_count}\")\n",
        "print(f\"Total time taken: {total_time} seconds\")\n"
      ],
      "metadata": {
        "id": "8FBL8NGV-puO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}